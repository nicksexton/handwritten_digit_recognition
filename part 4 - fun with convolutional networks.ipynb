{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Fun With Convolutional Networks\n",
    "\n",
    "In Parts 2 & 3 we applied a 3-layer perceptron (with 400, 150 hidden units) to the MNIST and Fashion-MNIST respectively. In terms of numbers of layers, this is probably the smallest network you could describe as 'deep learning' but as we've seen it does a decent job, giving a similar level of performance to state-of-the-art SVMs. \n",
    "\n",
    "Let's move on to a convolutional network. Given the significant increase in model complexity, we'd really hope to do a better job than the 3-layer perceptron and the SVM models. Quite often, you see Keras tutorials on this same dataset with very complex convolutional network (which Keras makes easy to do) but those models frequently don't actually get any better performance on the task than we've managed to do with the 3-layer perceptron, or could be done with other machine learning models, like an SVM (but with a massively increased training time).\n",
    "\n",
    "Let's build a couple of convolutional models and see see how our performance compares to the previous model on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nickdbn/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did previously (parts 2 and 3), we're going to import the MNIST and the Fashion MNIST in parallel. To be clear - we're going to train separate models on each dataset, but here we're loading and processing the data all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_x_train shape: (60000, 28, 28)\n",
      "f_y_train shape: (10000, 10)\n",
      "f_x_test shape: (60000, 28, 28)\n",
      "f_y_test shape: (10000, 10)\n",
      "d_x_train shape: (60000, 28, 28)\n",
      "d_y_train shape: (10000,)\n",
      "d_x_test shape: (60000, 28, 28)\n",
      "d_y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "(f_x_train, f_y_train_orig),(f_x_test, f_y_test_orig) = fashion_mnist.load_data()\n",
    "(d_x_train, d_y_train_orig), (d_x_test, d_y_test_orig) = mnist.load_data()\n",
    "\n",
    "f_x_train, f_x_test = f_x_train / 255.0, f_x_test / 255.0 # normalise\n",
    "d_x_train, d_x_test = d_x_train / 255.0, d_x_test / 255.0 # normalise\n",
    "\n",
    "# reshape y to arrays of shape (m,1)\n",
    "#f_y_train, f_y_test = f_y_train.reshape(f_y_train.shape[0], 1), f_y_test.reshape(f_y_test.shape[0], 1)\n",
    "#d_y_train, d_y_test = d_y_train.reshape(d_y_train.shape[0], 1), d_y_test.reshape(d_y_test.shape[0], 1)\n",
    "\n",
    "f_y_train = convert_to_one_hot(f_y_train_orig, 10).T\n",
    "f_y_test = convert_to_one_hot(f_y_test_orig, 10).T\n",
    "d_y_train = convert_to_one_hot(d_y_train_orig, 10).T\n",
    "d_y_test = convert_to_one_hot(f_y_test_orig, 10).T\n",
    "\n",
    "\n",
    "print (\"f_x_train shape:\", f_x_train.shape)\n",
    "print (\"f_y_train shape:\", f_y_test.shape)\n",
    "print (\"f_x_test shape:\", f_x_train.shape)\n",
    "print (\"f_y_test shape:\", f_y_test.shape)\n",
    "print (\"d_x_train shape:\", d_x_train.shape)\n",
    "print (\"d_y_train shape:\", d_y_test.shape)\n",
    "print (\"d_x_test shape:\", d_x_train.shape)\n",
    "print (\"d_y_test shape:\", d_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_examples_n = 40\n",
    "rows = 6\n",
    "cols = 8\n",
    "fig, axarr = plt.subplots(rows, cols)\n",
    "\n",
    "for r in range (0, rows):\n",
    "    for c in range (0, cols):\n",
    "        axarr[r][c].title.set_text(str(f_y_train[(r*cols)+c]))\n",
    "        axarr[r][c].imshow(f_x_train[(r*cols)+c])\n",
    "        \n",
    "for r in range (3, rows):\n",
    "    for c in range (0, cols):\n",
    "        axarr[r][c].title.set_text(str(d_y_train[(r*cols)+c]))\n",
    "        axarr[r][c].imshow(d_x_train[(r*cols)+c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this occasion, we're not going to bother carving the training set into a train/dev set. If we end up doing a lot of hyperparameter tuning we'll come back and do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_y_train_one_hot = convert_to_one_hot (f_y_train, 10)\n",
    "f_y_test_one_hot = convert_to_one_hot (f_y_test, 10)\n",
    "d_y_train_one_hot = convert_to_one_hot (d_y_train, 10)\n",
    "d_y_test_one_hot = convert_to_one_hot (d_y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"f_x_train shape:\", f_x_train.shape)\n",
    "print (\"f_y_train_one_hot shape:\", f_y_test.shape)\n",
    "print (\"f_x_test shape:\", f_x_train.shape)\n",
    "print (\"f_y_test_one_hot shape:\", f_y_test.shape)\n",
    "print (\"d_x_train shape:\", d_x_train.shape)\n",
    "print (\"d_y_train_one_hot shape:\", d_y_test.shape)\n",
    "print (\"d_x_test shape:\", d_x_train.shape)\n",
    "print (\"d_y_test_one_hot shape:\", d_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_y_train_one_hot.reshape(f_y_train_one_hot.shape[1], f_y_train_one_hot.shape[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_y_train_one_hot.reshape(f_y_train_one_hot.shape[1], f_y_train_one_hot.shape[0])[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates placeholders for input feature vector (X) and labels vector (Y) \n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of the input feature vector \n",
    "    n_y -- scalar, number of classes \n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(dtype=\"float\", shape=(n_x, None), name=\"X\")\n",
    "    Y = tf.placeholder(dtype=\"float\", shape=(n_y, None), name=\"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = create_placeholders(784, 10)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_3layers(size_input, size_l1, size_l2, size_l3):\n",
    "    \"\"\"\n",
    "    Initializes parameters for a 3 layer tensorflow neural network, where l1 is the \n",
    "    first hidden layer and l3 is the output layer. Uses Xavier initialization.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)  \n",
    "    \n",
    "    W1 = tf.get_variable (\"W1\", [size_l1,size_input], \n",
    "                          initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable (\"b1\", [size_l1,1], \n",
    "                          initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable (\"W2\", [size_l2,size_l1], \n",
    "                          initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable (\"b2\", [size_l2,1], \n",
    "                          initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable (\"W3\", [size_l3,size_l2], \n",
    "                          initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3 = tf.get_variable (\"b3\", [size_l3,1], \n",
    "                          initializer = tf.zeros_initializer())\n",
    "\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation_3layers(X, Y, parameters, beta = 0.01, keep_prob = 1.0):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for 4 layer network\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing parameters \"W1\", \"b1\", \"W2\", \"b2\", ... \"W5\", \"b5\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    cost -- softmax cross entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                          \n",
    "    A1 = tf.nn.relu(Z1)                \n",
    "\n",
    "    dropped1 = tf.nn.dropout(A1, keep_prob=keep_prob)\n",
    "    Z2 = tf.add(tf.matmul(W2, dropped1), b2)                     \n",
    "    A2 = tf.nn.relu(Z2)                \n",
    "    \n",
    "    dropped2 = tf.nn.dropout(A2, keep_prob=keep_prob)\n",
    "    Z3 = tf.add(tf.matmul(W3, dropped2), b3)                     \n",
    "\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)    \n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    regularizers = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W2) + tf.nn.l2_loss(W3) \n",
    "    cost = tf.reduce_mean(cost + beta * regularizers)\n",
    "        \n",
    "    return Z3, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the model\n",
    "Once again, we're using a three-layer model (i.e. a multi-layer perceptron), with layers l1 = 400, l2 = 150, l3 = 10, with l1 through l3 using relu units, and l4 a softmax layer. The parameters are trained via backprop optimising to softmax cross-entropy loss. \n",
    "\n",
    "L2 regularization didn't seem to help much on the digits dataset but who knows, let's leave it in here. The default setting of beta is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_training_by_epoch (costs, train_accuracy, test_accuracy, learning_rate):\n",
    "    \n",
    "    fig, ax1 = plt.subplots() \n",
    "    \n",
    "    # plot cost on primary y axis \n",
    "    cost = ax1.plot(np.squeeze(costs), label=\"cost\", color='b')\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('cost', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    \n",
    "    # plot accuracy on secondary y axis\n",
    "    ax2 = ax1.twinx()\n",
    "    train_accuracy = ax2.plot(np.squeeze(train_accuracy), \n",
    "                              label=\"train accuracy\", color='r', linestyle='-.')\n",
    "    test_accuracy = ax2.plot(np.squeeze(test_accuracy), \n",
    "                             label=\"test accuracy\", color='r', linestyle=':')\n",
    "    ax2.set_ylabel('accuracy', color='r')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, \n",
    "          layer_sizes = [784, 400, 150, 10], \n",
    "          learning_rate = 0.0001, # use eg 0.01 for Adagrad  \n",
    "          learn_algorithm = 'Adagrad',\n",
    "          beta = 0.0,\n",
    "          keep_prob = 1.0,\n",
    "          num_epochs = 50, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    for m, n-dimensional training examples, with c output classes \n",
    "    X_train -- training set, of shape (input size = n, number of training examples = m)\n",
    "    Y_train -- test set, of shape (output size = c, number of training examples = m)\n",
    "    X_test -- training set, of shape (input size = n, number of training examples = m)\n",
    "    Y_test -- test set, of shape (output size = c, number of test examples = m)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()           # to be able to rerun the model without \n",
    "                                        # overwriting tf variables\n",
    "#    tf.set_random_seed(1)               # to keep consistent results\n",
    "    (n_x, m) = X_train.shape            # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]              # n_y : output size\n",
    "    costs = []                          # To keep track of the cost (plotting per epoch)\n",
    "    train_accuracy = []                 # for plotting per epoch\n",
    "    test_accuracy = []                  # for plotting per epoch\n",
    "    \n",
    "    X, Y = create_placeholders (n_x, n_y)\n",
    "    parameters = initialize_parameters_3layers(size_input = layer_sizes[0], \n",
    "                                               size_l1 = layer_sizes[1], \n",
    "                                               size_l2 = layer_sizes[2], \n",
    "                                               size_l3 = layer_sizes[3])\n",
    "    # version of Z3 with dropout (for training)\n",
    "    Z3_train, cost = forward_propagation_3layers (X, Y, parameters, beta = beta, keep_prob = keep_prob)\n",
    "    \n",
    "    #Z3 with no dropout (for testing)\n",
    "    Z3, _ = forward_propagation_3layers (X, Y, parameters, beta = beta, keep_prob = 1.0) \n",
    "    \n",
    "    if learn_algorithm == 'Adagrad':\n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    else: \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "        \n",
    "    correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                           # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size \n",
    "                                                      # minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # the line that runs the optimizer\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, \n",
    "                                                                            Y:minibatch_Y})                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                timer = time.time() - tic\n",
    "                print (\"Cost after epoch %i: %f, running for %f seconds\" % (epoch, \n",
    "                                                                            epoch_cost, timer))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                # evaluate accuracy\n",
    "                train_accuracy.append (accuracy.eval({X: X_train, Y: Y_train}))\n",
    "                test_accuracy.append (accuracy.eval({X: X_test, Y: Y_test}))\n",
    "                \n",
    "        # plot the cost\n",
    "        plot_training_by_epoch (costs, train_accuracy, test_accuracy, learning_rate)\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "        timer = time.time() - tic\n",
    "        print (\"Execution time %f\" % timer)\n",
    "\n",
    "        print (\"Final Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Final Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters, costs, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs, train_accuracy, test_accuracy = model(train_set_X, train_set_Y_one_hot, \n",
    "                                                         dev_set_X, dev_set_Y_one_hot, \n",
    "                                                         learn_algorithm = 'Adam',\n",
    "                                                         learning_rate = 0.0001, \n",
    "                                                         beta = 0.0, keep_prob = 1.0,\n",
    "                                                         num_epochs = 60)\n",
    "                    # optimizer - Adam\n",
    "                    # default layer sizes - layer_sizes = [784, 400, 150, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                # just a reminder that we can now plot outside the model training function\n",
    "# plot_training_by_epoch (costs, train_accuracy, test_accuracy, learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for 20 epochs (about 5.5 minutes on my intel Core i5-3380M laptop with no GPU), the model gets to about 94% training accuracy and 89.7% test accuracy with a training cost of 0.17, with the cost function still decreasing suggesting training still has a fair way to go. To speed up training I think an adaptive learning rate may be beneficial.\n",
    "\n",
    "Trained for 60 epochs (about 15 minutes) made a substantial improvement in the training cost (0.06) resulting in a training accuracy of 98.8%. Interestingly, the model has a substantial variance problem, with the test accuracy reaching about 89% after about 10 epochs, and then essentially remaining static while the model over-fits to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (softmax: probability/confidence of each class)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\"     \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [784, None])  \n",
    "            # IMPORTANT Note we need to use None here as we \n",
    "            # don't know how many training\n",
    "            # examples will be passed in via feed_dict\n",
    "\n",
    "                                                           \n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      \n",
    "    A1 = tf.nn.relu(Z1)                                    \n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     \n",
    "    A2 = tf.nn.relu(Z2)                                    \n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     \n",
    "    \n",
    "    # Computes probabilities using forward propagation, \n",
    "    # and classifies to 0/1 using 0.5 as the threshold.\n",
    "  \n",
    "    p = tf.argmax(Z3)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_predictions_byclass (X, Y, num_classes = 10):\n",
    "\n",
    "    predict_set_X = X.astype(np.float32)\n",
    "    # print (\"predict_set_X:\\n\", predict_set_X)\n",
    "    # predict_set_X = predict_set_X.reshape(2,1) \n",
    "    # predict_set_X.shape\n",
    "    # type (predict_set_X[0,0]) # confirm type is float32\n",
    "\n",
    "    prediction = predict (predict_set_X, parameters)\n",
    "    print (\"The model predicted:\", prediction[0:9])\n",
    "    print (\"The correct labels are:\\n\", Y[0:9])\n",
    "    \n",
    "    evaluate_predictions = np.append(prediction.reshape(Y.shape), Y, axis=0)\n",
    "\n",
    "    # get a vector of True/False values\n",
    "    correct = (evaluate_predictions[0,:] == evaluate_predictions[1,:]) \n",
    "    print (correct[4000:4020])\n",
    "\n",
    "    classes = np.arange(0, num_classes)\n",
    "\n",
    "    # weight the bincount by True/False (ie only count correct responses)\n",
    "    correct_by_class = np.bincount(evaluate_predictions[0,:], weights=correct) \n",
    "    totals_by_class = np.bincount(evaluate_predictions[1,:])\n",
    "\n",
    "    accuracy_by_class = np.divide (correct_by_class, totals_by_class)\n",
    "    return accuracy_by_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_predictions_byclass (dev_set_X, dev_set_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model suffers from accuracy on particularly on classes 0, 2, 4 and 6. \n",
    "\n",
    "# Error analysis\n",
    "\n",
    "Let's see if we can get some insight into the types of errors the model is making and why, by visually inspecting some of the misclassified test items. First, we need to do a bit of data crunching on the vector of predictions (Y hat) and compare it to the correct answers (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_set_X = dev_set_X.astype(np.float32) # fix the data type\n",
    "\n",
    "prediction = predict (predict_set_X, parameters) # get the vector of predictions (i.e. Y hat)\n",
    "print (\"The model predicted:\", prediction[0:9])\n",
    "print (\"The correct labels are:\", dev_set_Y[0,0:9])\n",
    "    \n",
    "# bind the prediction vector (Y hat) with the correct label vector (Y)\n",
    "evaluate_predictions = np.append(prediction.reshape(dev_set_Y.shape), dev_set_Y, axis=0)\n",
    "\n",
    "# turn that into a vector of True/False values\n",
    "correct = (evaluate_predictions[0,:] == evaluate_predictions[1,:]) \n",
    "classes = np.arange(0, 10) # how many discrete classes do we have\n",
    "\n",
    "# weight the bincount by True/False (ie only count correct responses)\n",
    "correct_by_class = np.bincount(evaluate_predictions[0,:], weights=correct) \n",
    "totals_by_class = np.bincount(evaluate_predictions[1,:])\n",
    "\n",
    "accuracy_by_class = np.divide (correct_by_class, totals_by_class) # as a percentage\n",
    "print (accuracy_by_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look just at the errors on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"dev_set_Y.shape:\", dev_set_Y.shape)\n",
    "\n",
    "\n",
    "errors_Y = dev_set_Y[:,correct==False].T    # use the correct vector as a mask to make a vector of \n",
    "                                            #just the labels (Y) for incorrect predictions\n",
    "print (\"errors_Y.shape: \", errors_Y.shape)  # how many errors do we have?\n",
    "\n",
    "# use the correct mask to get a vector of incorrect predictions (Y hat)\n",
    "errors_Y_hat = prediction[correct==False].T \n",
    "errors_Y_hat = errors_Y_hat.reshape(errors_Y_hat.shape[0],1)\n",
    "print (\"errors_Y_hat.shape: \", errors_Y_hat.shape)\n",
    "\n",
    "# use the mask to get the images for the errors  \n",
    "errors_X = dev_set_X[:,correct==False]      \n",
    "print (\"errors_X.shape: \", errors_X.shape)\n",
    "\n",
    "# transpose and make back into a 28x28 image\n",
    "errors_X_2d = errors_X.T.reshape(errors_X.shape[1], 28, 28) \n",
    "print (\"errors_X_2d.shape: \", errors_X_2d.shape)\n",
    "\n",
    "rows = 4\n",
    "cols = 4\n",
    "fig, axarr = plt.subplots(rows, cols)\n",
    "\n",
    "for r in range (0, rows):\n",
    "    for c in range (0, cols):\n",
    "        axarr[r][c].title.set_text(\"y: \" + str(errors_Y[(r*cols)+c]) + \"y_hat: \" + str(errors_Y_hat[(r*cols)+c]) )\n",
    "        axarr[r][c].imshow(errors_X_2d[(r*cols)+c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## some ad-hoc hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can make any inroads into the variance problem. First of all, let's try adaptive learning rates using the AdagradOptimizer rather than Adam. (Because we want to try dropout, and Adam doesn't work with dropout) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_ada, costs_ada, train_accuracy_ada, test_accuracy_ada = model(train_set_X, \n",
    "                                                         train_set_Y_one_hot, \n",
    "                                                         dev_set_X, dev_set_Y_one_hot, \n",
    "                                                         learn_algorithm = 'Adagrad',\n",
    "                                                         learning_rate = 0.01, \n",
    "                                                         beta = 0.0, keep_prob = 1.0, \n",
    "                                                         num_epochs = 60)\n",
    "                    # optimizer - Adam\n",
    "                    # default layer sizes - layer_sizes = [784, 400, 150, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the model not appreciably quicker to train (both Adagrad and Adam are adaptive learning algorithms) \n",
    "\n",
    "## Todo plot cost/test accuracy of Adagrad vs. Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the Adagrad algorithm with dropout, to see if this helps fix the variance problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs, train_accuracy, test_accuracy = model(train_set_X, train_set_Y_one_hot, \n",
    "                                                         dev_set_X, dev_set_Y_one_hot, \n",
    "                                                         learn_algorithm = 'Adagrad',\n",
    "                                                         learning_rate = 0.01, \n",
    "                                                         beta = 0.0, keep_prob = 0.8,\n",
    "                                                         num_epochs = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing dropout (with a keep_prob of 0.8) does help a bit toward reducing overfitting (maybe 0.5% improvement in test set accuracy), at the cost of making training take longer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dropout AND L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs, train_accuracy, test_accuracy = model(train_set_X, train_set_Y_one_hot, \n",
    "                                                         dev_set_X, dev_set_Y_one_hot, \n",
    "                                                         learn_algorithm = 'Adagrad',\n",
    "                                                         learning_rate = 0.01, \n",
    "                                                         beta = 0.001, keep_prob = 0.8,\n",
    "                                                         num_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
