{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - the MNIST dataset\n",
    "\n",
    "For ease of use, the MINST handwritten digits dataset comes packaged up with TensorFlow, so let's load it and have a look around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "y_train shape: (10000, 1)\n",
      "x_test shape: (60000, 28, 28)\n",
      "y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# reshape y to arrays of shape (m,1)\n",
    "y_train, y_test = y_train.reshape(y_train.shape[0], 1), y_test.reshape(y_test.shape[0], 1)\n",
    "\n",
    "print (\"x_train shape:\", x_train.shape)\n",
    "print (\"y_train shape:\", y_test.shape)\n",
    "print (\"x_test shape:\", x_train.shape)\n",
    "print (\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualise some examples of the training set with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_examples_n = 40\n",
    "rows = 5\n",
    "cols = 8\n",
    "fig, axarr = plt.subplots(rows, cols)\n",
    "\n",
    "for r in range (0, rows):\n",
    "    for c in range (0, cols):\n",
    "        # axarr[r][c].title.set_text('x_train[' + str((r*cols)+c) +']: ' + str(y_train[(r*cols)+c]))\n",
    "        axarr[r][c].title.set_text(str(y_train[(r*cols)+c]))\n",
    "        axarr[r][c].imshow(x_train[(r*cols)+c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's look at the number of items in each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique (y_train, return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would probably be better if the training set had equal numbers of items in each class, and we wrote code that does this in carving up the set into a train and dev set. No need to split off a test set, as we already have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique (y_test, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_dev_sets (X, Y, train_set_proportion = 0.9):\n",
    "    \"\"\"\n",
    "    Takes set of features (X) as (n,m) matrix and labels (Y) as (1,m) matrix and splits them into train and dev \n",
    "    sets only, of specified proportions. It's assumed that a test set isn't needed.\n",
    "    Train set will have same number of examples of each class\n",
    "    \n",
    "    returns: train_set_X, train_set_Y, dev_set_X, dev_set_Y\n",
    "    \"\"\"\n",
    "    \n",
    "    size_classes = np.unique(Y, return_counts = True)[1] # get an array of all class sizes\n",
    "\n",
    "    # our training set contains train_set_proportion * smallest class size of each class\n",
    "    size_smallest_class = min (np.unique (Y, return_counts = True)[1]) \n",
    "    size_train_set_class = int (train_set_proportion * size_smallest_class)\n",
    "    print (\"size_train_set_class:\", size_train_set_class)\n",
    "    # print (\"size_dev_set_class:\", size_dev_set_class)\n",
    "    \n",
    "    \n",
    "    num_classes = np.shape(np.unique(Y))[0]\n",
    "\n",
    "    size_classes_cum = np.empty ((0))\n",
    "    for i in range (0, num_classes): # get an array of cumulative indices, starting with 0, for where each class starts\n",
    "        size_classes_cum = np.append (size_classes_cum, int (sum(size_classes[0:i])))\n",
    "    # add on final size of the data set +1 so we can iterate i+1 over num_classes to get end indices\n",
    "    size_classes_cum = np.append (size_classes_cum, int(Y.shape[1]))\n",
    "    \n",
    "    \n",
    "    #print (\"num classes: \", num_classes)\n",
    "    #print (\"size_classes_cum: \", size_classes_cum)\n",
    "    #print (\"dtype size_classes_cum:\", type (size_classes_cum))\n",
    "    \n",
    "    sorted_indices = np.argsort (Y[0,:]) # get the list of indices that will sort Y by class\n",
    "    # print (\"sorted_indices.shape:\", sorted_indices.shape)\n",
    "    X_sorted = X[:, sorted_indices]\n",
    "    Y_sorted = Y[:, sorted_indices] \n",
    "    \n",
    "\n",
    "    \n",
    "    # initialise sets\n",
    "    train_set_X = np.empty ((X.shape[0], 0))\n",
    "    train_set_Y = np.empty ((1, 0))\n",
    "    dev_set_X = np.empty ((X.shape[0], 0))\n",
    "    dev_set_Y = np.empty ((1, 0))\n",
    "    \n",
    "    # print (\"train_set_X.shape before append:\", train_set_X.shape)\n",
    "    # print (\"train_set_Y.shape before append:\", train_set_Y.shape)\n",
    "    \n",
    "    for i in range (0, num_classes):\n",
    "        # print (\"size_classes_cum[i]:\", size_classes_cum[i])\n",
    "        # print (\"size_classes_cum[i]+size_train_set_class:\", size_classes_cum[i]+size_train_set_class)\n",
    "        X_this_class = X_sorted[:, int (size_classes_cum[i]):int (size_classes_cum[i]+size_train_set_class)]\n",
    "        # print (\"X_this_class shape:\", X_this_class.shape)\n",
    "        train_set_X = np.append (train_set_X, X_sorted[:, int (size_classes_cum[i]):int (size_classes_cum[i]+size_train_set_class)], axis=1)\n",
    "        train_set_Y = np.append (train_set_Y, Y_sorted[:, int (size_classes_cum[i]):int (size_classes_cum[i]+size_train_set_class)], axis=1)\n",
    "        \n",
    "        dev_set_X = np.append (dev_set_X, X_sorted[:, int (size_classes_cum[i]+size_train_set_class):int(size_classes_cum[i+1])], axis=1)\n",
    "        dev_set_Y = np.append (dev_set_Y, Y_sorted[:, int (size_classes_cum[i]+size_train_set_class):int(size_classes_cum[i+1])], axis=1)\n",
    "\n",
    "       \n",
    "    # Finally, apply the same shuffle to X and Y sets\n",
    "    train_shuffled_indices = np.arange (train_set_X.shape[1])\n",
    "    dev_shuffled_indices = np.arange (dev_set_X.shape[1])\n",
    "\n",
    "    np.random.shuffle (train_shuffled_indices)\n",
    "    np.random.shuffle (dev_shuffled_indices)\n",
    "\n",
    "    train_set_X = train_set_X[:,train_shuffled_indices]\n",
    "    train_set_Y = train_set_Y[:,train_shuffled_indices].astype (np.int16)    \n",
    "    dev_set_X = dev_set_X[:,dev_shuffled_indices]\n",
    "    dev_set_Y = dev_set_Y[:,dev_shuffled_indices].astype (np.int16)    \n",
    "    \n",
    "    return train_set_X, train_set_Y, dev_set_X, dev_set_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "y_train shape: (60000, 1)\n",
      "x_test shape: (10000, 28, 28)\n",
      "y_test shape: (10000, 1)\n",
      "x_train_unrow shape: (60000, 784)\n",
      "x_test_unrow shape: (10000, 784)\n",
      "x_train_trans shape: (784, 60000)\n",
      "y_train_trans shape: (1, 60000)\n",
      "x_test_trans shape: (784, 10000)\n",
      "y_test_trans shape: (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "print (\"x_train shape:\", x_train.shape)\n",
    "print (\"y_train shape:\", y_train.shape)\n",
    "print (\"x_test shape:\", x_test.shape)\n",
    "print (\"y_test shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "x_train_unrow = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
    "x_test_unrow = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])\n",
    "\n",
    "print (\"x_train_unrow shape:\", x_train_unrow.shape)\n",
    "print (\"x_test_unrow shape:\", x_test_unrow.shape)\n",
    "\n",
    "x_train_trans = x_train_unrow.T\n",
    "x_test_trans = x_test_unrow.T\n",
    "y_train_trans = y_train.T\n",
    "y_test_trans = y_test.T\n",
    "print (\"x_train_trans shape:\", x_train_trans.shape)\n",
    "print (\"y_train_trans shape:\", y_train_trans.shape)\n",
    "print (\"x_test_trans shape:\", x_test_trans.shape)\n",
    "print (\"y_test_trans shape:\", y_test_trans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_train_set_class: 4878\n",
      "train_set_X.shape:  (784, 48780)\n",
      "train_set_Y.shape:  (1, 48780)\n",
      "dev_set_X.shape: (784, 11220)\n",
      "dev_set_Y.shape: (1, 11220)\n",
      "execution time: 2.342122 s\n"
     ]
    }
   ],
   "source": [
    "tic=time.time()\n",
    "train_set_X, train_set_Y, dev_set_X, dev_set_Y = get_train_dev_sets (x_train_trans, y_train_trans)\n",
    "\n",
    "#train_set_X = train_set_X.T\n",
    "#train_set_Y = train_set_Y.T\n",
    "#dev_set_X = dev_set_X.T\n",
    "#dev_set_Y = dev_set_Y.T\n",
    "\n",
    "print (\"train_set_X.shape: \", train_set_X.shape)\n",
    "print (\"train_set_Y.shape: \", train_set_Y.shape)\n",
    "print (\"dev_set_X.shape:\", dev_set_X.shape)\n",
    "print (\"dev_set_Y.shape:\", dev_set_Y.shape)\n",
    "timer = time.time() - tic\n",
    "print (\"execution time: %f s\" % timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int16),\n",
       " array([4878, 4878, 4878, 4878, 4878, 4878, 4878, 4878, 4878, 4878]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique (train_set_Y, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C): \n",
    "    \"\"\"\n",
    "    Converts a vector Y of multiclass-responses coded as integer values (0, 1, 2, etc.... C-1) \n",
    "    to a (C,m) dimensional matrix where each row represents a response, and values are either 0 or 1\n",
    "    \"\"\"\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 6 6 3 1 2 1 0 9 8]]\n",
      "...converts to:\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print (train_set_Y[:,0:10])\n",
    "\n",
    "train_set_Y_one_hot = convert_to_one_hot (train_set_Y,  10)\n",
    "dev_set_Y_one_hot = convert_to_one_hot (dev_set_Y,  10)\n",
    "print (\"...converts to:\")\n",
    "print (train_set_Y_one_hot[:, 0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to make and train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates placeholders for input feature vector (X) and labels vector (Y) \n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of the input feature vector \n",
    "    n_y -- scalar, number of classes \n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(dtype=\"float\", shape=(n_x, None), name=\"X\")\n",
    "    Y = tf.placeholder(dtype=\"float\", shape=(n_y, None), name=\"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(784, ?), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(10, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(784, 10)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_3layers(size_input, size_l1, size_l2, size_l3):\n",
    "    \"\"\"\n",
    "    Initializes parameters for a 3 layer tensorflow neural network, where l1 is the first hidden layer \n",
    "    and l3 is the output layer.\n",
    "    Uses Xavier initialization.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 6 lines of code)\n",
    "    W1 = tf.get_variable (\"W1\", [size_l1,size_input], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b1 = tf.get_variable (\"b1\", [size_l1,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable (\"W2\", [size_l2,size_l1], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b2 = tf.get_variable (\"b2\", [size_l2,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable (\"W3\", [size_l3,size_l2], initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    b3 = tf.get_variable (\"b3\", [size_l3,1], initializer = tf.zeros_initializer())\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation_3layers(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)                     # Z3 = np.dot(W3,Z2) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)    \n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    X, Y = create_placeholders (n_x, n_y)\n",
    "    parameters = initialize_parameters_3layers(size_input = 784, size_l1 = 400, size_l2 = 150, size_l3 = 10)\n",
    "    Z3 = forward_propagation_3layers (X, parameters)\n",
    "    cost = compute_cost (Z3, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    tic = time.time()\n",
    "    \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # the line that runs the optimizer\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 50 == 0:\n",
    "                timer = time.time() - tic\n",
    "                print (\"Cost after epoch %i: %f, running for %f\" % (epoch, epoch_cost, timer))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "        timer = time.time() - tic\n",
    "        print (\"Execution time %f\" % timer)\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.312588, running for 5.534426\n"
     ]
    }
   ],
   "source": [
    "parameters = model(train_set_X, train_set_Y_one_hot, dev_set_X, dev_set_Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1539958802.2399511"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
